{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57cb10b8-6556-487a-80d5-7f5b165b996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnykralafk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\sneep\\Python\\bbf\\wandb\\run-20240303_005106-l3yn8ct8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/snykralafk/Atari-100k-MoE/runs/l3yn8ct8' target=\"_blank\">BBF MoE Assault</a></strong> to <a href='https://wandb.ai/snykralafk/Atari-100k-MoE' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/snykralafk/Atari-100k-MoE' target=\"_blank\">https://wandb.ai/snykralafk/Atari-100k-MoE</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/snykralafk/Atari-100k-MoE/runs/l3yn8ct8' target=\"_blank\">https://wandb.ai/snykralafk/Atari-100k-MoE/runs/l3yn8ct8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count, chain\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n",
    "from nosaveddata.nsd_utils.networks import params_count, params_and_grad_norm, seed_np_torch\n",
    "from nosaveddata.nsd_utils.einstein import Rearrange\n",
    "from nosaveddata.builders.weight_init import *\n",
    "\n",
    "from nosaveddata.builders.mlp import *\n",
    "from nosaveddata.builders.resnet import IMPALA_Resnet, DQN_Conv\n",
    "from nosaveddata.builders.token_learner import TokenLearner\n",
    "from nosaveddata.builders.moe import SoftMoE_Combine_Output\n",
    "\n",
    "\n",
    "from utils.experience_replay import *\n",
    "\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Environment configuration\n",
    "env_name = 'Assault'\n",
    "SEED = 7783\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Atari-100k-MoE\",\n",
    "    name=f\"BBF MoE {env_name}\",\n",
    "\n",
    "    #id='rotdmtc5',\n",
    "    #resume='must',\n",
    "\n",
    "    config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"architecture\": \"BBF\",\n",
    "        \"dataset\": \"Assault\",\n",
    "        \"epochs\": 100,\n",
    "    },\n",
    "\n",
    "    reinit=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Optimization\n",
    "batch_size = 32\n",
    "lr=1e-4\n",
    "\n",
    "eps=1e-8\n",
    "\n",
    "\n",
    "# Target network EMA rate\n",
    "critic_ema_decay=0.995\n",
    "#critic_ema_decay=0.98\n",
    "\n",
    "\n",
    "# Return function\n",
    "initial_gamma=torch.tensor(1-0.97).log()\n",
    "final_gamma=torch.tensor(1-0.997).log()\n",
    "\n",
    "initial_n = 10\n",
    "final_n = 3\n",
    "\n",
    "num_buckets=51\n",
    "\n",
    "\n",
    "# Reset Schedule and Buffer\n",
    "reset_every=40000 # grad steps, not steps.\n",
    "replay_ratio=2\n",
    "schedule_max_step=reset_every//4\n",
    "total_steps=100000\n",
    "\n",
    "prefetch_cap=1 # actually, no prefetch is being done\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'reward', 'action', 'c_flag'))\n",
    "memory = PrioritizedReplay_nSteps_Sqrt(100005, total_steps=schedule_max_step, prefetch_cap=prefetch_cap)\n",
    "\n",
    "\n",
    "\n",
    "def save_checkpoint(net, model_target, optimizer, step, path):\n",
    "    torch.save({\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'model_target_state_dict': model_target.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'step': step,\n",
    "            }, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c831907e-0868-4915-bc34-3d1dd1fe363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\gymnasium\\vector\\__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/weipu-zhang/STORM/blob/main/env_wrapper.py\n",
    "class MaxLast2FrameSkipWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4, seed=0):\n",
    "        super().__init__(env)\n",
    "        self.seed = seed\n",
    "        self.skip = skip\n",
    "        self.env.action_space.seed(seed)\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        kwargs[\"seed\"] = self.seed\n",
    "        obs, _ = self.env.reset(**kwargs)\n",
    "        return obs, _\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        self.obs_buffer = deque(maxlen=2)\n",
    "        for _ in range(self.skip):\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            self.obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "\n",
    "            terminated = np.logical_or(done, truncated)\n",
    "            #if terminated.any():\n",
    "            #    for i in range(len(terminated)):\n",
    "            #       obs[i] = self.reset()[0][i]\n",
    "            if done or truncated:\n",
    "                break\n",
    "        if len(self.obs_buffer) == 1:\n",
    "            obs = self.obs_buffer[0]\n",
    "        else:\n",
    "            obs = np.max(np.stack(self.obs_buffer), axis=0)\n",
    "        return obs, total_reward, done, truncated, info\n",
    "        # Life loss is calculated on the training code\n",
    "\n",
    "env = gym.vector.make(f\"{env_name}NoFrameskip-v4\", num_envs=1)\n",
    "env = MaxLast2FrameSkipWrapper(env,seed=SEED)\n",
    "\n",
    "\n",
    "#n_actions = env.action_space.n\n",
    "n_actions = env.action_space[0].n\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "seed_np_torch(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d429f2-aee0-4c6b-b221-d2a8b09b0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n",
      "DQN Parameters: 7.16M\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n",
      "DQN Parameters: 7.16M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def renormalize(tensor, has_batch=False):\n",
    "    shape = tensor.shape\n",
    "    tensor = tensor.view(tensor.shape[0], -1)\n",
    "    max_value,_ = torch.max(tensor, -1, keepdim=True)\n",
    "    min_value,_ = torch.min(tensor, -1, keepdim=True)\n",
    "    return ((tensor - min_value) / (max_value - min_value + 1e-5)).view(shape)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_actions, hiddens=2048, mlp_layers=1, scale_width=4,\n",
    "                 n_atoms=51, Vmin=-10, Vmax=10):\n",
    "        super().__init__()\n",
    "        self.support = torch.linspace(Vmin, Vmax, n_atoms).cuda()\n",
    "        \n",
    "        self.hiddens=hiddens\n",
    "        self.scale_width=scale_width\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.encoder_cnn = IMPALA_Resnet(scale_width=scale_width, norm=False, init=init_xavier, act=self.act)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Single layer dense that maps the flattened encoded representation into hiddens.\n",
    "        self.rearrange = Rearrange('b c h w -> b (h w) c')\n",
    "        self.moe = SoftMoE_Combine_Output(32*scale_width, hiddens, num_experts=4, num_slots=1)\n",
    "        self.prediction = MLP(hiddens, out_hiddens=hiddens, layers=1, last_init=init_xavier)\n",
    "                                              \n",
    "        self.transition = nn.Sequential(DQN_Conv(128+n_actions, 128, 3, 1, 1, norm=False, init=init_xavier, act=self.act),\n",
    "                                        DQN_Conv(128, 128, 3, 1, 1, norm=False, init=init_xavier, act=self.act))\n",
    "\n",
    "        # Single layer dense that maps hiddens into the output dim according to:\n",
    "        # 1. https://arxiv.org/pdf/1707.06887.pdf -- Distributional Reinforcement Learning\n",
    "        # 2. https://arxiv.org/pdf/1511.06581.pdf -- Dueling DQN\n",
    "        self.a = MLP(hiddens, out_hiddens=n_actions*num_buckets, layers=1, in_act=self.act, last_init=init_xavier)\n",
    "        self.v = MLP(hiddens, out_hiddens=num_buckets, layers=1, in_act=self.act, last_init=init_xavier)\n",
    "    \n",
    "        params_count(self, 'DQN')\n",
    "    \n",
    "    def forward(self, X, y_action):\n",
    "        X, z, _ = self.encode(X)\n",
    "        \n",
    "        \n",
    "        q, action = self.q_head(X)\n",
    "        z_pred, combine_w = self.get_transition(z, y_action)\n",
    "\n",
    "        return q, action, X[:,1:].clone().detach(), z_pred, combine_w\n",
    "    \n",
    "\n",
    "    def env_step(self, X):\n",
    "        with torch.no_grad():\n",
    "            X, _, _ = self.encode(X)\n",
    "            _, action = self.q_head(X)\n",
    "            \n",
    "            return action.detach()\n",
    "    \n",
    "\n",
    "    def encode(self, X):\n",
    "        batch, seq = X.shape[:2]\n",
    "        self.batch = batch\n",
    "        self.seq = seq\n",
    "        X = self.encoder_cnn(X.contiguous().view(self.batch*self.seq, *(X.shape[2:])))\n",
    "        X = renormalize(X).contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        X = X.contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        z = X.clone()\n",
    "        X, combine_w = self.moe(self.rearrange(X.view(self.batch*self.seq, *X.shape[-3:])))\n",
    "        X = X.view(self.batch*self.seq, -1)\n",
    "        return X, z, combine_w\n",
    "\n",
    "    def get_transition(self, z, action):\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action = F.one_hot(action.clone(), n_actions).view(-1, n_actions)\n",
    "        action = action.view(-1, 5, n_actions, 1, 1).expand(-1, 5, n_actions, *z.shape[-2:])\n",
    "\n",
    "        \n",
    "        z_pred = torch.cat( (z, action[:,0]), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        z_pred = renormalize(z_pred)\n",
    "        \n",
    "        z_preds=[z_pred.clone()]\n",
    "        \n",
    "\n",
    "        for k in range(4):\n",
    "            z_pred = torch.cat( (z_pred, action[:,k+1]), 1)\n",
    "            z_pred = self.transition(z_pred)\n",
    "            z_pred = renormalize(z_pred)\n",
    "            \n",
    "            z_preds.append(z_pred)\n",
    "        \n",
    "        \n",
    "        z_pred = torch.stack(z_preds,1)\n",
    "\n",
    "        z_pred, combine_w = self.moe(self.rearrange(z_pred.view(-1,*z_pred.shape[-3:])))\n",
    "        z_pred = self.prediction(z_pred.view(self.batch,5,-1))\n",
    "        \n",
    "        return z_pred, combine_w.view(self.batch,5,-1)\n",
    "\n",
    "    \n",
    "    def q_head(self, X):\n",
    "        q = self.dueling_dqn(X)\n",
    "        action = (q*self.support).sum(-1).argmax(-1)\n",
    "        \n",
    "        return q, action\n",
    "\n",
    "    def get_max_action(self, X):\n",
    "        with torch.no_grad():\n",
    "            X, _, _ = self.encode(X)\n",
    "            q = self.dueling_dqn(X)\n",
    "            \n",
    "            action = (q*self.support).sum(-1).argmax(-1)\n",
    "            return action\n",
    "\n",
    "    def evaluate(self, X, action):\n",
    "        with torch.no_grad():\n",
    "            X, _, _ = self.encode(X)\n",
    "            \n",
    "            q = self.dueling_dqn(X)\n",
    "            \n",
    "            action = action[:,:,None,None].expand_as(q)[:,:,0][:,:,None]\n",
    "            q = q.gather(-2,action)\n",
    "            \n",
    "            return q\n",
    "\n",
    "    def dueling_dqn(self, X):\n",
    "        X = F.relu(X)\n",
    "        \n",
    "        a = self.a(X).view(self.batch, -1, n_actions, num_buckets)\n",
    "        v = self.v(X).view(self.batch, -1, 1, num_buckets)\n",
    "        \n",
    "        q = v + a - a.mean(-2,keepdim=True)\n",
    "        q = F.softmax(q,-1)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def network_ema(self, rand_network, target_network, alpha=0.5):\n",
    "        for param, param_target in zip(rand_network.parameters(), target_network.parameters()):\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param.data.clone()\n",
    "\n",
    "    def hard_reset(self, random_model, alpha=0.5):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            self.network_ema(random_model.encoder_cnn, self.encoder_cnn, alpha)\n",
    "            self.network_ema(random_model.transition, self.transition, alpha)\n",
    "\n",
    "            self.network_ema(random_model.moe, self.moe, 0)\n",
    "            self.network_ema(random_model.prediction, self.prediction, 0)\n",
    "\n",
    "            self.network_ema(random_model.a, self.a, 0)\n",
    "            self.network_ema(random_model.v, self.v, 0)\n",
    "\n",
    "\n",
    "\n",
    "def copy_states(source, target):\n",
    "    for key, _ in zip(source.state_dict()['state'].keys(), target.state_dict()['state'].keys()):\n",
    "        target.state_dict()['state'][key]['exp_avg_sq'] = copy.deepcopy(source.state_dict()['state'][key]['exp_avg_sq'])\n",
    "        target.state_dict()['state'][key]['exp_avg'] = copy.deepcopy(source.state_dict()['state'][key]['exp_avg'])\n",
    "        target.state_dict()['state'][key]['step'] = copy.deepcopy(source.state_dict()['state'][key]['step'])\n",
    "        \n",
    "def target_model_ema(model, model_target):\n",
    "    with torch.no_grad():\n",
    "        for param, param_target in zip(model.parameters(), model_target.parameters()):\n",
    "            param_target.data = critic_ema_decay * param_target.data + (1.0 - critic_ema_decay) * param.data.clone()\n",
    "\n",
    "\n",
    "    \n",
    "model=DQN(n_actions).cuda()\n",
    "model_target=DQN(n_actions).cuda()\n",
    "\n",
    "model_target.load_state_dict(model.state_dict())\n",
    "\n",
    "\n",
    "# Testing only\n",
    "with torch.no_grad():\n",
    "    q, action, X, z_pred, combine_w = model(torch.randn(4,1,12,96,72, device='cuda', dtype=torch.float), torch.randint(0,n_actions,(4,5),device='cuda').long())\n",
    "#z = model.encode(torch.randn(4,5,12,96,72, device='cuda'))[0]\n",
    "\n",
    "# I believe the authors have actually miscalculated the params count on the paper.\n",
    "# My training time is lower than theirs while having more parameters, and the same architecture is used as is their original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ca1d0e5-d128-428b-bcbc-be68dccc005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception_modules=[model.encoder_cnn, model.transition]\n",
    "actor_modules=[model.prediction, model.moe, model.a, model.v]\n",
    "\n",
    "params_wm=[]\n",
    "for module in perception_modules:\n",
    "    for param in module.parameters():\n",
    "        if param.requires_grad==True:\n",
    "            params_wm.append(param)\n",
    "\n",
    "params_ac=[]\n",
    "for module in actor_modules:\n",
    "    for param in module.parameters():\n",
    "        if param.requires_grad==True:\n",
    "            params_ac.append(param)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(chain(params_wm, params_ac),\n",
    "                                lr=lr, weight_decay=0.1, eps=1.5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7f2a7d7-81c6-4c93-8cbe-1f15fa75af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "                         transforms.Resize((96,72)),\n",
    "                        ])\n",
    "\n",
    "\n",
    "def preprocess(state):\n",
    "    state=torch.tensor(state, dtype=torch.float, device='cuda') / 255\n",
    "    state=train_tfms(state.permute(0,3,1,2))\n",
    "    return state\n",
    "\n",
    "# https://github.com/google/dopamine/blob/master/dopamine/jax/agents/dqn/dqn_agent.py\n",
    "def linearly_decaying_epsilon(decay_period, step, warmup_steps, epsilon):\n",
    "    steps_left = decay_period + warmup_steps - step\n",
    "    bonus = (1.0 - epsilon) * steps_left / decay_period\n",
    "    bonus = np.clip(bonus, 0., 1. - epsilon)\n",
    "    return epsilon + bonus\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q_action, step, final_eps=0, num_envs=1):\n",
    "    epsilon = linearly_decaying_epsilon(2001, step, 2000, final_eps)\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        action = torch.randint(0, n_actions, (num_envs,), dtype=torch.int64, device='cuda').squeeze(0)\n",
    "    else:\n",
    "        action = Q_action.view(num_envs).squeeze(0).to(torch.int64)\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6722829c-d4bc-4d9f-aa35-80f6133a4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/google/dopamine/blob/master/dopamine/jax/agents/rainbow/rainbow_agent.py\n",
    "def project_distribution(supports, weights, target_support):\n",
    "    with torch.no_grad():\n",
    "        v_min, v_max = target_support[0], target_support[-1]\n",
    "        # `N` in Eq7.\n",
    "        num_dims = target_support.shape[-1]\n",
    "        # delta_z = `\\Delta z` in Eq7.\n",
    "        delta_z = (v_max - v_min) / (num_buckets-1)\n",
    "        # clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.\n",
    "        clipped_support = supports.clip(v_min, v_max)\n",
    "        # numerator = `|clipped_support - z_i|` in Eq7.\n",
    "        numerator = (clipped_support[:,None] - target_support[None,:,None].repeat_interleave(clipped_support.shape[0],0)).abs()\n",
    "        quotient = 1 - (numerator / delta_z)\n",
    "        # clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.\n",
    "        clipped_quotient = quotient.clip(0, 1)\n",
    "        # inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))` in Eq7.\n",
    "        inner_prod = (clipped_quotient * weights[:,None]).sum(-1)\n",
    "        #inner_prod = (clipped_quotient).sum(-1) * weights\n",
    "        return inner_prod.squeeze()\n",
    "\n",
    "\n",
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "def optimize(step, grad_step, n):\n",
    "        \n",
    "    model.train()\n",
    "    model_target.train()\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=False):\n",
    "        with torch.no_grad():\n",
    "            states, next_states, rewards, action, c_flag, idxs, is_w = memory.sample(n, batch_size, grad_step)\n",
    "        terminal=1-c_flag\n",
    "        #print(f\"STUFF HERE {states.shape, rewards.shape, c_flag.shape, action.shape, n}\")\n",
    "    \n",
    "    \n",
    "        q, max_action, _, z_pred, combine_w_pred = model(states[:,0][:,None], action[:,:5].long())\n",
    "        with torch.no_grad():\n",
    "            z, _, combine_w_target = model_target.encode(states[:,1:6])\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        max_action  = model.get_max_action(next_states[:,n-1][:,None])\n",
    "        next_values = model_target.evaluate(next_states[:,n-1][:,None].contiguous(), max_action)\n",
    "        \n",
    "\n",
    "        action = action[:,0,None].expand(batch_size,num_buckets)\n",
    "        action=action[:,None]\n",
    "        with torch.no_grad():\n",
    "            gammas_one=torch.ones(batch_size,n,1,dtype=torch.float,device='cuda')\n",
    "            gamma_step = 1-torch.tensor(( (schedule_max_step - min(grad_step, schedule_max_step)) / schedule_max_step) * (initial_gamma-final_gamma) + final_gamma).exp()\n",
    "            gammas=gammas_one*gamma_step\n",
    "\n",
    "            \n",
    "            returns = []\n",
    "            for t in range(n):\n",
    "                ret = 0\n",
    "                for u in reversed(range(t, n)):\n",
    "                    ret += torch.prod(c_flag[:,t+1:u+1],-2)*torch.prod(gammas[:,t:u],-2)*rewards[:,u+1]\n",
    "                returns.append(ret)\n",
    "            returns = torch.stack(returns,1)\n",
    "        \n",
    "        plot_vs = returns.clone().sum(-1)\n",
    "        \n",
    "        same_traj = (torch.prod(c_flag[:,:n],-2)).squeeze()\n",
    "        \n",
    "        returns = returns[:,0]\n",
    "        returns = returns + torch.prod(gammas[0,:10],-2).squeeze()*same_traj[:,None]*model.support[None,:]\n",
    "        returns = returns.squeeze()\n",
    "        \n",
    "        next_values = next_values[:,0]\n",
    "\n",
    "        log_probs = torch.log(q[:,0].gather(-2, action)[:,None] + eps).contiguous()\n",
    "        \n",
    "        \n",
    "        dist = project_distribution(returns, next_values.squeeze(), model.support)\n",
    "\n",
    "        loss = -(dist*(log_probs.squeeze())).sum(-1).view(batch_size,-1).sum(-1)\n",
    "        dqn_loss = loss.clone().mean()\n",
    "        td_error = (loss + torch.nan_to_num((dist*torch.log(dist))).sum(-1)).mean()\n",
    "\n",
    "        \n",
    "        batched_loss = loss.clone()\n",
    "        \n",
    "        \n",
    "        z = F.normalize(z, 2, dim=-1, eps=1e-5)\n",
    "        z_pred = F.normalize(z_pred, 2, dim=-1, eps=1e-5)\n",
    "\n",
    "        \n",
    "        recon_loss = (mse(z_pred.contiguous().view(-1,2048), z.contiguous().view(-1,2048))).sum(-1)\n",
    "        recon_loss = recon_loss.view(batch_size, -1)\n",
    "        combine_w_target = combine_w_target.view(batch_size, 5, -1)\n",
    "        rho = ((combine_w_pred.log() - combine_w_target.log()).abs().clip(0,1)).mean(-1)\n",
    "        recon_loss = recon_loss#*rho\n",
    "        recon_loss = 5*(recon_loss.view(batch_size, -1).mean(-1))*same_traj\n",
    "\n",
    "        \n",
    "        loss += recon_loss\n",
    "        \n",
    "        loss = (loss*is_w).mean() # mean across batch axis\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    param_norm, grad_norm = params_and_grad_norm(model)\n",
    "    #scaler.scale(loss).backward()\n",
    "    #scaler.unscale_(optimizer)\n",
    "    #torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "    #scaler.step(optimizer)\n",
    "    #scaler.update()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #memory.set_priority(idxs, batched_loss)\n",
    "    memory.set_priority(idxs, batched_loss, same_traj)\n",
    "    \n",
    "    \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    wandb.log({'loss': loss, 'dqn_loss': dqn_loss, 'recon_loss': recon_loss.mean(), 'lr': lr, 'returns': plot_vs.mean(),\n",
    "               'is_w': is_w.mean(), 'td_error': td_error, 'param_norm': param_norm.sum(), 'grad_norm': grad_norm.sum(),\n",
    "               'rho': rho.mean()})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "scores=[]\n",
    "memory.free()\n",
    "step=0\n",
    "#model.share_memory()\n",
    "grad_step=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05991348-a11a-43bd-882f-02df107fc543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1992/100000 [00:07<07:09, 228.45it/s]C:\\Users\\sneep\\AppData\\Local\\Temp\\ipykernel_10768\\3296822170.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gamma_step = 1-torch.tensor(( (schedule_max_step - min(grad_step, schedule_max_step)) / schedule_max_step) * (initial_gamma-final_gamma) + final_gamma).exp()\n",
      " 22%|██▏       | 22002/100000 [40:09<2:34:25,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseting on step 22001 40002\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22003/100000 [40:10<4:47:23,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Parameters: 7.16M\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n",
      "DQN Parameters: 7.16M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42003/100000 [1:22:15<2:01:53,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseting on step 42002 40002\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n",
      "DQN Parameters: 7.16M\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42005/100000 [1:22:15<3:55:31,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Parameters: 7.16M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62004/100000 [2:06:02<1:21:02,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseting on step 62003 40002\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n",
      "DQN Parameters: 7.16M\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62006/100000 [2:06:03<3:12:18,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Parameters: 7.16M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82005/100000 [2:52:11<41:53,  7.16it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reseting on step 82004 40002\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82006/100000 [2:52:12<2:27:41,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Parameters: 7.16M\n",
      "IMPALA ResNet Parameters: 1.56M\n",
      "Soft MoE Parameters: 0.27M\n",
      "DQN Parameters: 7.16M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [3:39:00<00:00,  6.16it/s] "
     ]
    }
   ],
   "source": [
    "step=0\n",
    "\n",
    "progress_bar = tqdm.tqdm(total=total_steps)\n",
    "\n",
    "while step<(10):\n",
    "    state, info = env.reset()\n",
    "    state = preprocess(state)\n",
    "\n",
    "    stacked = state.repeat_interleave(3,1)\n",
    "    state = torch.cat((stacked.clone(), state),1)\n",
    "    \n",
    "    \n",
    "    eps_reward=torch.tensor([0], dtype=torch.float)\n",
    "    \n",
    "    reward=np.array([0])\n",
    "    done_flag=np.array([False])\n",
    "    terminated=np.array([False])\n",
    "\n",
    "    last_lives=np.array([0])\n",
    "    life_loss=np.array([0])\n",
    "    \n",
    "    last_grad_update=0\n",
    "    while step<(total_steps):\n",
    "        progress_bar.update(1)\n",
    "        model_target.train()\n",
    "        \n",
    "        len_memory = len(memory)\n",
    "        Q_action = model_target.env_step(state.unsqueeze(0))\n",
    "        \n",
    "        action = epsilon_greedy(Q_action, len_memory).cpu()\n",
    "        \n",
    "        memory.push(state.detach().cpu(), reward, action, np.logical_or(done_flag, life_loss))\n",
    "        #print('action', action, action.shape)\n",
    "        \n",
    "        state, reward, terminated, truncated, info = env.step([action.numpy()])\n",
    "        \n",
    "        state = preprocess(state)\n",
    "        eps_reward+=reward\n",
    "        reward = reward.clip(-1, 1)\n",
    "\n",
    "        state = torch.cat((stacked.clone(), state),1)\n",
    "        stacked = torch.cat((stacked[:,3:], state[:,-3:]),1)\n",
    "\n",
    "        \n",
    "        done_flag = np.logical_or(terminated, truncated)\n",
    "        lives = info['lives']\n",
    "        life_loss = (last_lives-lives).clip(min=0)\n",
    "        last_lives = lives\n",
    "\n",
    "        \n",
    "        n = int(initial_n * (final_n/initial_n)**(min(grad_step,schedule_max_step) / schedule_max_step))\n",
    "        n = np.array(n).item()\n",
    "        \n",
    "        memory.priority[len_memory] = memory.max_priority()\n",
    "        \n",
    "\n",
    "        if len_memory>2000:\n",
    "            for i in range(replay_ratio):\n",
    "                optimize(step, grad_step, n)\n",
    "                target_model_ema(model, model_target)\n",
    "                grad_step+=1\n",
    "\n",
    "        \n",
    "        if ((step+1)%10000)==0:\n",
    "            save_checkpoint(model, model_target, optimizer, step,\n",
    "                            'checkpoints/moe_atari_last.pth')\n",
    "        \n",
    "            \n",
    "        \n",
    "        if grad_step>reset_every:\n",
    "            #eval()\n",
    "            print('Reseting on step', step, grad_step)\n",
    "            \n",
    "            seed_np_torch(random.randint(SEED-1000, SEED+1000)+step)\n",
    "            random_model = DQN(n_actions).cuda()\n",
    "            model.hard_reset(random_model)\n",
    "            \n",
    "            seed_np_torch(random.randint(SEED-1000, SEED+1000)+step)\n",
    "            random_model = DQN(n_actions).cuda()\n",
    "            model_target.hard_reset(random_model)\n",
    "            seed_np_torch(SEED)\n",
    "            \n",
    "            random_model=None\n",
    "            grad_step=0\n",
    "\n",
    "            actor_modules=[model.prediction, model.moe, model.a, model.v]\n",
    "            params_ac=[]\n",
    "            for module in actor_modules:\n",
    "                for param in module.parameters():\n",
    "                    params_ac.append(param)\n",
    "                    \n",
    "\n",
    "            perception_modules=[model.encoder_cnn, model.transition]\n",
    "            params_wm=[]\n",
    "            for module in perception_modules:\n",
    "                for param in module.parameters():\n",
    "                    params_wm.append(param)\n",
    "            \n",
    "            optimizer_aux = torch.optim.AdamW(params_wm, lr=lr, weight_decay=0.1, eps=1.5e-4)\n",
    "            copy_states(optimizer, optimizer_aux)\n",
    "            optimizer = torch.optim.AdamW(chain(params_wm, params_ac),\n",
    "                                lr=lr, weight_decay=0.1, eps=1.5e-4)\n",
    "            copy_states(optimizer_aux, optimizer)\n",
    "            \n",
    "        \n",
    "        \n",
    "        step+=1\n",
    "        \n",
    "        log_t = done_flag.astype(float).nonzero()[0]\n",
    "        \n",
    "        if len(log_t)>0:\n",
    "            for log in log_t:\n",
    "                wandb.log({'eps_reward': eps_reward[log].sum()})\n",
    "                scores.append(eps_reward[log].clone())\n",
    "            eps_reward[log_t]=0\n",
    "\n",
    "save_checkpoint(model, model_target, optimizer, step, f'checkpoints/moe_{env_name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87344597-e733-451c-8a5f-2a00b5511061",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_eval=True\n",
    "\n",
    "if load_to_eval:\n",
    "    model.load_state_dict(torch.load(f'checkpoints/moe_{env_name}.pth')['model_state_dict'])\n",
    "    model_target.load_state_dict(torch.load(f'checkpoints/moe_{env_name}.pth')['model_target_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d36218-ab48-42d9-a7ba-fabb66cc758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\gymnasium\\vector\\__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init state torch.Size([1, 3, 96, 72])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 1/100 [00:02<03:44,  2.27s/it]\u001b[A\n",
      "  2%|▏         | 2/100 [00:04<03:57,  2.43s/it]\u001b[A\n",
      "  3%|▎         | 3/100 [00:07<03:53,  2.41s/it]\u001b[A\n",
      "  4%|▍         | 4/100 [00:09<03:46,  2.36s/it]\u001b[A\n",
      "  5%|▌         | 5/100 [00:11<03:49,  2.41s/it]\u001b[A\n",
      "  6%|▌         | 6/100 [00:14<03:34,  2.28s/it]\u001b[A\n",
      "  7%|▋         | 7/100 [00:16<03:27,  2.23s/it]\u001b[A\n",
      "  8%|▊         | 8/100 [00:18<03:22,  2.20s/it]\u001b[A\n",
      "  9%|▉         | 9/100 [00:20<03:11,  2.10s/it]\u001b[A\n",
      " 10%|█         | 10/100 [00:22<03:11,  2.13s/it]\u001b[A\n",
      " 11%|█         | 11/100 [00:24<03:22,  2.28s/it]\u001b[A\n",
      " 12%|█▏        | 12/100 [00:27<03:30,  2.39s/it]\u001b[A\n",
      " 13%|█▎        | 13/100 [00:29<03:23,  2.34s/it]\u001b[A\n",
      " 14%|█▍        | 14/100 [00:32<03:19,  2.32s/it]\u001b[A\n",
      " 15%|█▌        | 15/100 [00:34<03:18,  2.33s/it]\u001b[A\n",
      " 16%|█▌        | 16/100 [00:37<03:20,  2.39s/it]\u001b[A\n",
      " 17%|█▋        | 17/100 [00:38<02:56,  2.12s/it]\u001b[A\n",
      " 18%|█▊        | 18/100 [00:40<02:48,  2.06s/it]\u001b[A\n",
      " 19%|█▉        | 19/100 [00:42<02:56,  2.19s/it]\u001b[A\n",
      " 20%|██        | 20/100 [00:45<03:05,  2.32s/it]\u001b[A\n",
      " 21%|██        | 21/100 [00:47<03:03,  2.32s/it]\u001b[A\n",
      " 22%|██▏       | 22/100 [00:50<02:59,  2.31s/it]\u001b[A\n",
      " 23%|██▎       | 23/100 [00:52<03:10,  2.48s/it]\u001b[A\n",
      " 24%|██▍       | 24/100 [00:55<03:06,  2.45s/it]\u001b[A\n",
      " 25%|██▌       | 25/100 [00:57<02:52,  2.30s/it]\u001b[A\n",
      " 26%|██▌       | 26/100 [01:00<03:10,  2.58s/it]\u001b[A\n",
      " 27%|██▋       | 27/100 [01:02<03:01,  2.49s/it]\u001b[A\n",
      " 28%|██▊       | 28/100 [01:05<02:56,  2.45s/it]\u001b[A\n",
      " 29%|██▉       | 29/100 [01:07<02:55,  2.47s/it]\u001b[A\n",
      " 30%|███       | 30/100 [01:09<02:48,  2.40s/it]\u001b[A\n",
      " 31%|███       | 31/100 [01:12<02:42,  2.35s/it]\u001b[A\n",
      " 32%|███▏      | 32/100 [01:14<02:42,  2.39s/it]\u001b[A\n",
      " 33%|███▎      | 33/100 [01:17<02:43,  2.44s/it]\u001b[A\n",
      " 34%|███▍      | 34/100 [01:19<02:41,  2.45s/it]\u001b[A\n",
      " 35%|███▌      | 35/100 [01:22<02:39,  2.46s/it]\u001b[A\n",
      " 36%|███▌      | 36/100 [01:24<02:33,  2.40s/it]\u001b[A\n",
      " 37%|███▋      | 37/100 [01:26<02:33,  2.44s/it]\u001b[A\n",
      " 38%|███▊      | 38/100 [01:29<02:31,  2.45s/it]\u001b[A\n",
      " 39%|███▉      | 39/100 [01:31<02:25,  2.38s/it]\u001b[A\n",
      " 40%|████      | 40/100 [01:34<02:27,  2.46s/it]\u001b[A\n",
      " 41%|████      | 41/100 [01:36<02:22,  2.42s/it]\u001b[A\n",
      " 42%|████▏     | 42/100 [01:39<02:23,  2.48s/it]\u001b[A\n",
      " 43%|████▎     | 43/100 [01:41<02:14,  2.36s/it]\u001b[A\n",
      " 44%|████▍     | 44/100 [01:43<02:08,  2.29s/it]\u001b[A\n",
      " 45%|████▌     | 45/100 [01:45<02:02,  2.22s/it]\u001b[A\n",
      " 46%|████▌     | 46/100 [01:47<02:01,  2.26s/it]\u001b[A\n",
      " 47%|████▋     | 47/100 [01:49<01:54,  2.16s/it]\u001b[A\n",
      " 48%|████▊     | 48/100 [01:52<01:54,  2.21s/it]\u001b[A\n",
      " 49%|████▉     | 49/100 [01:54<01:51,  2.18s/it]\u001b[A\n",
      " 50%|█████     | 50/100 [01:56<01:48,  2.17s/it]\u001b[A\n",
      " 51%|█████     | 51/100 [01:59<01:52,  2.30s/it]\u001b[A\n",
      " 52%|█████▏    | 52/100 [02:01<01:49,  2.29s/it]\u001b[A\n",
      " 53%|█████▎    | 53/100 [02:03<01:51,  2.38s/it]\u001b[A\n",
      " 54%|█████▍    | 54/100 [02:06<01:50,  2.40s/it]\u001b[A\n",
      " 55%|█████▌    | 55/100 [02:08<01:47,  2.38s/it]\u001b[A\n",
      " 56%|█████▌    | 56/100 [02:11<01:53,  2.58s/it]\u001b[A\n",
      " 57%|█████▋    | 57/100 [02:14<01:55,  2.69s/it]\u001b[A\n",
      " 58%|█████▊    | 58/100 [02:17<01:49,  2.60s/it]\u001b[A\n",
      " 59%|█████▉    | 59/100 [02:18<01:37,  2.37s/it]\u001b[A\n",
      " 60%|██████    | 60/100 [02:21<01:33,  2.33s/it]\u001b[A\n",
      " 61%|██████    | 61/100 [02:23<01:31,  2.34s/it]\u001b[A\n",
      " 62%|██████▏   | 62/100 [02:25<01:26,  2.27s/it]\u001b[A\n",
      " 63%|██████▎   | 63/100 [02:27<01:24,  2.27s/it]\u001b[A\n",
      " 64%|██████▍   | 64/100 [02:30<01:21,  2.25s/it]\u001b[A\n",
      " 65%|██████▌   | 65/100 [02:32<01:23,  2.37s/it]\u001b[A\n",
      " 66%|██████▌   | 66/100 [02:35<01:23,  2.46s/it]\u001b[A\n",
      " 67%|██████▋   | 67/100 [02:38<01:25,  2.59s/it]\u001b[A\n",
      " 68%|██████▊   | 68/100 [02:40<01:22,  2.59s/it]\u001b[A\n",
      " 69%|██████▉   | 69/100 [02:43<01:19,  2.55s/it]\u001b[A\n",
      " 70%|███████   | 70/100 [02:45<01:16,  2.55s/it]\u001b[A\n",
      " 71%|███████   | 71/100 [02:48<01:14,  2.58s/it]\u001b[A\n",
      " 72%|███████▏  | 72/100 [02:50<01:07,  2.43s/it]\u001b[A\n",
      " 73%|███████▎  | 73/100 [02:53<01:07,  2.48s/it]\u001b[A\n",
      " 74%|███████▍  | 74/100 [02:55<01:03,  2.42s/it]\u001b[A\n",
      " 75%|███████▌  | 75/100 [02:57<01:00,  2.43s/it]\u001b[A\n",
      " 76%|███████▌  | 76/100 [03:00<00:58,  2.45s/it]\u001b[A\n",
      " 77%|███████▋  | 77/100 [03:03<00:58,  2.57s/it]\u001b[A\n",
      " 78%|███████▊  | 78/100 [03:06<00:59,  2.72s/it]\u001b[A\n",
      " 79%|███████▉  | 79/100 [03:08<00:55,  2.64s/it]\u001b[A\n",
      " 80%|████████  | 80/100 [03:12<00:56,  2.81s/it]\u001b[A\n",
      " 81%|████████  | 81/100 [03:14<00:51,  2.71s/it]\u001b[A\n",
      " 82%|████████▏ | 82/100 [03:16<00:46,  2.59s/it]\u001b[A\n",
      " 83%|████████▎ | 83/100 [03:19<00:42,  2.48s/it]\u001b[A\n",
      " 84%|████████▍ | 84/100 [03:21<00:38,  2.38s/it]\u001b[A\n",
      " 85%|████████▌ | 85/100 [03:23<00:34,  2.31s/it]\u001b[A\n",
      " 86%|████████▌ | 86/100 [03:25<00:32,  2.29s/it]\u001b[A\n",
      " 87%|████████▋ | 87/100 [03:28<00:30,  2.34s/it]\u001b[A\n",
      " 88%|████████▊ | 88/100 [03:30<00:27,  2.32s/it]\u001b[A\n",
      " 89%|████████▉ | 89/100 [03:32<00:24,  2.27s/it]\u001b[A\n",
      " 90%|█████████ | 90/100 [03:35<00:25,  2.56s/it]\u001b[A\n",
      " 91%|█████████ | 91/100 [03:38<00:23,  2.61s/it]\u001b[A\n",
      " 92%|█████████▏| 92/100 [03:40<00:20,  2.59s/it]\u001b[A\n",
      " 93%|█████████▎| 93/100 [03:43<00:17,  2.51s/it]\u001b[A\n",
      " 94%|█████████▍| 94/100 [03:45<00:14,  2.34s/it]\u001b[A\n",
      " 95%|█████████▌| 95/100 [03:47<00:12,  2.43s/it]\u001b[A\n",
      " 96%|█████████▌| 96/100 [03:50<00:10,  2.63s/it]\u001b[A\n",
      " 97%|█████████▋| 97/100 [03:53<00:07,  2.56s/it]\u001b[A\n",
      " 98%|█████████▊| 98/100 [03:56<00:05,  2.62s/it]\u001b[A\n",
      " 99%|█████████▉| 99/100 [03:58<00:02,  2.44s/it]\u001b[A\n",
      "100%|██████████| 100/100 [03:59<00:00,  2.40s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores Mean 380.73\n",
      "Inter Quantile Mean 374.64\n",
      "Inter Quantile STD 30.40526270236783\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_envs=1\n",
    "\n",
    "#env = gym.vector.make(f\"{env_name}NoFrameskip-v4\", num_envs=num_envs, render_mode='human')\n",
    "env = gym.vector.make(f\"{env_name}NoFrameskip-v4\", num_envs=num_envs)\n",
    "env = MaxLast2FrameSkipWrapper(env,seed=SEED)\n",
    "\n",
    "def eval_phase(eval_runs=50, max_eval_steps=27000, num_envs=1):\n",
    "    progress_bar = tqdm.tqdm(total=eval_runs)\n",
    "    \n",
    "    scores=[]\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    state = preprocess(state)\n",
    "    print(f\"init state {state.shape}\")\n",
    "    stacked = state.repeat_interleave(3,1)\n",
    "    state = torch.cat((stacked.clone(), state),1)\n",
    "    \n",
    "    \n",
    "    eps_reward=torch.tensor([0]*num_envs, dtype=torch.float)\n",
    "    \n",
    "    reward=np.array([0]*num_envs)\n",
    "    terminated=np.array([False]*num_envs)\n",
    "\n",
    "    last_lives=np.array([0]*num_envs)\n",
    "\n",
    "    finished_envs=np.array([False]*num_envs)\n",
    "    done_flag=0\n",
    "    last_grad_update=0\n",
    "    eval_run=0\n",
    "    step=np.array([0]*num_envs)\n",
    "    while eval_run<eval_runs:\n",
    "        #seed_np_torch(SEED+eval_run)\n",
    "        env.seed=SEED+eval_run\n",
    "        model_target.train()\n",
    "        \n",
    "        \n",
    "        Q_action = model_target.env_step(state.unsqueeze(0))\n",
    "        action = epsilon_greedy(Q_action.squeeze(), 5000, 0.0005, num_envs).cpu()\n",
    "        \n",
    "        state, reward, terminated, truncated, info = env.step([action.numpy()] if num_envs==1 else action.numpy())\n",
    "        done_flag = np.logical_or(terminated, truncated)\n",
    "        \n",
    "\n",
    "        state = preprocess(state)\n",
    "        eps_reward+=reward\n",
    "\n",
    "\n",
    "        \n",
    "        state = torch.cat((stacked.clone(), state),1)\n",
    "        stacked = torch.cat((stacked[:,3:], state[:,-3:]),1)\n",
    "        \n",
    "        \n",
    "        step+=1\n",
    "        \n",
    "        log_t = done_flag.astype(float).nonzero()[0]\n",
    "        if len(log_t)>0:# or (step>max_eval_steps).any():\n",
    "            progress_bar.update(1)\n",
    "            for log in log_t:\n",
    "                #wandb.log({'eval_eps_reward': eps_reward[log].sum()})\n",
    "                if finished_envs[log]==False:\n",
    "                    scores.append(eps_reward[log].clone())\n",
    "                    eval_run+=1\n",
    "                    #finished_envs[log]=True\n",
    "                step[log]=0\n",
    "                \n",
    "            eps_reward[log_t]=0            \n",
    "            for i, log in enumerate(step>max_eval_steps):\n",
    "                if log==True and finished_envs[i]==False:\n",
    "                    scores.append(eps_reward[i].clone())\n",
    "                    step[i]=0\n",
    "                    eval_run+=1\n",
    "                    eps_reward[i]=0\n",
    "                    #finished_envs[i]=True\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def eval(eval_runs=50, max_eval_steps=27000, num_envs=1):\n",
    "    assert num_envs==1, 'The code for num eval envs > 1 is messed up.'\n",
    "    \n",
    "    scores = eval_phase(eval_runs, max_eval_steps, num_envs)    \n",
    "    scores = torch.stack(scores)\n",
    "    scores, _ = scores.sort()\n",
    "    \n",
    "    _25th = eval_runs//4\n",
    "\n",
    "    iq = scores[_25th:-_25th]\n",
    "    iqm = iq.mean()\n",
    "    iqs = iq.std()\n",
    "\n",
    "    print(f\"Scores Mean {scores.mean()}\")\n",
    "    print(f\"Inter Quantile Mean {iqm}\")\n",
    "    print(f\"Inter Quantile STD {iqs}\")\n",
    "\n",
    "    with open(f'results/MoE-{env_name}-{SEED}.txt', 'w') as f:\n",
    "        f.write(f\" Scores Mean {scores.mean()}\\n Inter Quantile Mean {iqm}\\n Inter Quantile STD {iqs}\")\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(scores)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = eval(eval_runs=100, num_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cddb4f-7fdd-4b4b-8e62-33d92f4313df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
