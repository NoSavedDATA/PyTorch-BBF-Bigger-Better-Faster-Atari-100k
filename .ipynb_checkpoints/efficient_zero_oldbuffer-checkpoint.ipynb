{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57cb10b8-6556-487a-80d5-7f5b165b996a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\diffusers\\utils\\outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msnykralafk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3407be4ffa654e1991fb0ab84301d1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\sneep\\Python\\bbf\\wandb\\run-20240421_181931-7igy33xz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/snykralafk/Atari-100k-Efficient%20Zero/runs/7igy33xz' target=\"_blank\">EffZ-Assault</a></strong> to <a href='https://wandb.ai/snykralafk/Atari-100k-Efficient%20Zero' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/snykralafk/Atari-100k-Efficient%20Zero' target=\"_blank\">https://wandb.ai/snykralafk/Atari-100k-Efficient%20Zero</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/snykralafk/Atari-100k-Efficient%20Zero/runs/7igy33xz' target=\"_blank\">https://wandb.ai/snykralafk/Atari-100k-Efficient%20Zero/runs/7igy33xz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count, chain\n",
    "import tqdm\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from nosaveddata.nsd_utils.save_hypers import Hypers, nsd_Module\n",
    "from nosaveddata.nsd_utils.nsd_csv import add_to_csv\n",
    "from nosaveddata.nsd_utils.networks import params_count, params_and_grad_norm, seed_np_torch\n",
    "from nosaveddata.nsd_utils.einstein import Rearrange\n",
    "from nosaveddata.nsd_utils.dreamer import symlog, symexp, two_hot, two_hot_view, two_hot_view_no_symlog, ReturnsNormalizer\n",
    "\n",
    "from nosaveddata.builders.mlp import *\n",
    "from nosaveddata.builders.weight_init import *\n",
    "from nosaveddata.builders.resnet import IMPALA_Resnet, DQN_Conv, IMPALA_YY, Residual_Block\n",
    "\n",
    "from utils.experience_replay import *\n",
    "\n",
    "\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Environment configuration\n",
    "#env_name = 'Kangaroo'\n",
    "#SEED = 8712\n",
    "\n",
    "env_name = 'Assault'\n",
    "SEED = 7783\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Atari-100k-Efficient Zero\",\n",
    "    name=f\"EffZ-{env_name}\",\n",
    "\n",
    "    #id='rotdmtc5',\n",
    "    #resume='must',\n",
    "\n",
    "    config={\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"architecture\": \"Efficient Zero\",\n",
    "        \"dataset\": \"Assault\",\n",
    "        \"epochs\": 100,\n",
    "    },\n",
    "\n",
    "    reinit=False\n",
    ")\n",
    "\n",
    "\n",
    "# Optimization\n",
    "batch_size = 256\n",
    "lr=3e-4\n",
    "\n",
    "eps=1e-8\n",
    "\n",
    "\n",
    "# Target network EMA rate\n",
    "#critic_ema_decay=0.995\n",
    "critic_ema_decay=0.98\n",
    "#critic_ema_decay=0.95\n",
    "\n",
    "\n",
    "# Return function\n",
    "initial_gamma=torch.tensor(1-0.97).log()\n",
    "final_gamma=torch.tensor(1-0.997).log()\n",
    "\n",
    "#initial_n = 10\n",
    "#final_n = 3\n",
    "\n",
    "initial_n = 5\n",
    "final_n = 5\n",
    "\n",
    "num_buckets=51\n",
    "\n",
    "n_sim = 16\n",
    "topk_actions = 8\n",
    "\n",
    "\n",
    "# Reset Schedule and Buffer\n",
    "#reset_every=40000 # grad steps, not steps.\n",
    "reset_every=100000 # grad steps, not steps.\n",
    "schedule_max_step=reset_every//4\n",
    "total_steps=102000\n",
    "\n",
    "prefetch_cap=4 # actually, no prefetch is being done\n",
    "\n",
    "\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'reward', 'action', 'c_flag'))\n",
    "memory = PrioritizedReplay_nSteps_Sqrt(total_steps+5, total_steps=schedule_max_step, prefetch_cap=prefetch_cap, alpha=1, beta=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "returns_normalizer = ReturnsNormalizer(0.99)\n",
    "\n",
    "\n",
    "def save_checkpoint(net, model_target, optimizer, step, path):\n",
    "    torch.save({\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'model_target_state_dict': model_target.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'step': step,\n",
    "            }, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c831907e-0868-4915-bc34-3d1dd1fe363c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sneep\\anaconda3\\envs\\python_\\Lib\\site-packages\\gymnasium\\vector\\__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N° of actions: 7.\n"
     ]
    }
   ],
   "source": [
    "# Adapted from: https://github.com/weipu-zhang/STORM/blob/main/env_wrapper.py\n",
    "class MaxLast2FrameSkipWrapper(Hypers, gym.Wrapper):\n",
    "    def __init__(self, env, skip=4, noops=30, seed=0):\n",
    "        super().__init__(env=env)\n",
    "        self.env.action_space.seed(seed)\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        kwargs[\"seed\"] = self.seed\n",
    "        obs, _ = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs, _\n",
    "        \n",
    "    def noop_steps(self, states):\n",
    "        noops = random.randint(0,self.noops)\n",
    "        \n",
    "        for i in range(noops):\n",
    "            state = self.step(np.array([0]))[0]\n",
    "            state = preprocess(state)\n",
    "            states.append(state)\n",
    "        return states\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0\n",
    "        self.obs_buffer = deque(maxlen=2)\n",
    "        for _ in range(self.skip):\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            self.obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "\n",
    "            terminated = np.logical_or(done, truncated)\n",
    "            #if terminated.any():\n",
    "            #    for i in range(len(terminated)):\n",
    "            #       obs[i] = self.reset()[0][i]\n",
    "            if done or truncated:\n",
    "                break\n",
    "        if len(self.obs_buffer) == 1:\n",
    "            obs = self.obs_buffer[0]\n",
    "        else:\n",
    "            obs = np.max(np.stack(self.obs_buffer), axis=0)\n",
    "        return obs, total_reward, done, truncated, info\n",
    "        # Life loss is calculated on the training code\n",
    "\n",
    "env = gym.vector.make(f\"{env_name}NoFrameskip-v4\", num_envs=1)\n",
    "env = MaxLast2FrameSkipWrapper(env,seed=SEED)\n",
    "\n",
    "\n",
    "#n_actions = env.action_space.n\n",
    "n_actions = env.action_space[0].n\n",
    "\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "\n",
    "\n",
    "seed_np_torch(SEED)\n",
    "\n",
    "print(f\"N° of actions: {n_actions}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d429f2-aee0-4c6b-b221-d2a8b09b0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficient Zero Network Parameters: 5.80M\n",
      "Efficient Zero Network Parameters: 5.80M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class EffZ_Perception(nsd_Module):\n",
    "    def __init__(self, n_actions, scale_width=1, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(12, 32, 3, stride=2, padding=1, bias=False),\n",
    "                                   nn.BatchNorm2d(32),\n",
    "                                   act)\n",
    "        self.conv2 = Residual_Block(32, 32, act=act, out_act=act)\n",
    "        self.conv3 = Residual_Block(32, 64, act=act, out_act=act, stride=2)\n",
    "        self.conv4 = Residual_Block(64, 64, act=act, out_act=act)\n",
    "        self.pool1 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        self.conv5 = Residual_Block(64, 64, act=act, out_act=act)\n",
    "        self.pool2 = nn.AvgPool2d(3, stride=2, padding=1)\n",
    "        self.conv6 = Residual_Block(64, 64, act=act, out_act=act)\n",
    "        \n",
    "        self.conv1.apply(init_xavier)\n",
    "        \n",
    "        self.conv = nn.Sequential(self.conv1, self.conv2, self.conv3, self.conv4, self.pool1,\n",
    "                                   self.conv5, self.pool2, self.conv6)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.conv(X)\n",
    "        return X\n",
    "\n",
    "class _1conv_residual(nn.Module):\n",
    "    def __init__(self, hiddens, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(nn.Conv2d(hiddens+1, hiddens, 3, padding=1, bias=False),\n",
    "                                        nn.BatchNorm2d(hiddens))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        proj = x[:,:-1]\n",
    "        x = self.net(x)\n",
    "        \n",
    "        return x+proj\n",
    "        \n",
    "class RewardPred(nsd_Module):\n",
    "    def __init__(self, in_channels, out_channels, in_hiddens, hiddens, bottleneck=32, out_dim=51, act=nn.ReLU(), k=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 act)\n",
    "        \n",
    "        self.lstm = nn.LSTMCell(in_hiddens, hiddens)\n",
    "        self.norm_relu = nn.Sequential(nn.BatchNorm1d(hiddens))\n",
    "        \n",
    "        self.mlp = MLP_LayerNorm(hiddens, bottleneck, out_dim, layers=2, in_act=act, init=init_xavier, last_init=init_zeros, out_act=nn.Softmax(-1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, seq = x.shape[:2]\n",
    "        \n",
    "        x = self.conv(x.view(bs*seq, *x.shape[-3:])).view(bs,seq,-1)\n",
    "        \n",
    "        ht = torch.zeros(x.shape[0], self.hiddens, device='cuda')\n",
    "        ct = torch.zeros_like(ht)\n",
    "        \n",
    "        hs = []\n",
    "        for i in range(self.k):\n",
    "            \n",
    "            ht, ct = self.lstm(x[:,i], (ht, ct))\n",
    "            hs.append(ht)\n",
    "        hs = torch.stack(hs,1)\n",
    "        \n",
    "        x = self.mlp(hs)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def transition_one_step(self, x, ht):\n",
    "        \n",
    "        x = self.conv(x).view(x.shape[0],-1)\n",
    "        #print('reward one step', x.shape)\n",
    "        \n",
    "        ht, ct = self.lstm(x, ht)\n",
    "        \n",
    "        x = self.mlp(ht)\n",
    "        \n",
    "        return x, (ht,ct)\n",
    "        \n",
    "\n",
    "class ActorCritic(nsd_Module):\n",
    "    def __init__(self, in_channels, out_channels, in_hiddens, bottleneck=32, out_value=51, out_policy=1, act=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.residual = Residual_Block(in_channels, in_channels, act=self.act, out_act=self.act)\n",
    "        \n",
    "        conv_policy = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 act)\n",
    "        conv_value  = nn.Sequential(nn.Conv2d(in_channels, out_channels, 1),\n",
    "                                 nn.BatchNorm2d(out_channels),\n",
    "                                 act)\n",
    "        \n",
    "        self.policy = nn.Sequential(conv_policy,\n",
    "                                    nn.Flatten(-3,-1),\n",
    "                                   MLP_LayerNorm(in_hiddens, bottleneck, out_policy, layers=2, in_act=act, init=init_xavier, last_init=init_zeros))\n",
    "        self.value = nn.Sequential(conv_policy,\n",
    "                                    nn.Flatten(-3,-1),\n",
    "                                   MLP_LayerNorm(in_hiddens, bottleneck, out_value,  layers=2, in_act=act,\n",
    "                                                 out_act=nn.Softmax(dim=-1), init=init_xavier, last_init=init_zeros))\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, seq = x.shape[:2]\n",
    "        \n",
    "        x = self.residual(x.view(-1, *x.shape[-3:]))\n",
    "        \n",
    "        logits = self.policy(x).view(bs, seq, -1)\n",
    "        probs = F.softmax(logits, -1)\n",
    "        \n",
    "        value_probs = self.value(x).view(bs, seq, -1)\n",
    "        \n",
    "        return logits, probs, value_probs\n",
    "        \n",
    "    def one_step(self, x):\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        x = self.residual(x.view(-1, *x.shape[-3:]))\n",
    "        \n",
    "        logits = self.policy(x).view(bs, -1)\n",
    "        probs = F.softmax(logits, -1)\n",
    "        \n",
    "        value_probs = self.value(x).view(bs, -1)\n",
    "        \n",
    "        return logits, probs, value_probs\n",
    "        \n",
    "    \n",
    "class EfficientZero(nsd_Module):\n",
    "    def __init__(self, n_actions, hiddens=512, mlp_layers=1, scale_width=1,\n",
    "                 n_atoms=51, Vmin=-20, Vmax=20):\n",
    "        super().__init__()\n",
    "        self.support = torch.linspace(Vmin, Vmax, n_atoms).cuda()\n",
    "        self.reward_support = torch.linspace(-2, 2, n_atoms).cuda()\n",
    "        \n",
    "        self.hiddens=hiddens\n",
    "        self.scale_width=scale_width\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        #self.encoder_cnn = IMPALA_Resnet(scale_width=scale_width, norm=False, init=init_xavier, act=self.act)\n",
    "        self.encoder_cnn = EffZ_Perception(n_actions, scale_width)\n",
    "        \n",
    "\n",
    "        self.projection = MLP_LayerNorm(2304*scale_width, hiddens, hiddens*2,\n",
    "                                        init=init_xavier, last_init=init_xavier, layers=3, in_act=self.act,\n",
    "                                        add_last_norm=False)\n",
    "        self.prediction = MLP_LayerNorm(hiddens*2, hiddens, hiddens*2, layers=2,\n",
    "                                        init=init_xavier, last_init=init_xavier,\n",
    "                                        in_act=self.act, add_last_norm=False)\n",
    "        \n",
    "                                       \n",
    "            \n",
    "        self.transition = nn.Sequential(_1conv_residual(64, self.act),\n",
    "                                        Residual_Block(64, 64, act=self.act, out_act=self.act))\n",
    "        \n",
    "        self.reward_pred = RewardPred(64, 16, 16*((96//16)**2), hiddens)\n",
    "        #self.reward_pred = RewardPred(64, 16, 576, hiddens)\n",
    "\n",
    "        \n",
    "        self.ac = ActorCritic(64, 16, 16*((96//16)**2), out_policy=n_actions)\n",
    "    \n",
    "        params_count(self, 'Efficient Zero Network')\n",
    "        \n",
    "    \n",
    "    def forward(self, X, y_action):\n",
    "        z_proj, z = self.encode(X)\n",
    "        \n",
    "        \n",
    "        #q, action = self.q_head(X)\n",
    "        logits, probs, value_probs = self.ac(z)\n",
    "        \n",
    "        z_proj_pred, reward_pred = self.get_transition(z[:,0][:,None], y_action)\n",
    "\n",
    "        #return q, action, X[:,1:].clone().detach(), z_pred\n",
    "        return z_proj, z_proj_pred, reward_pred, logits, probs, value_probs\n",
    "    \n",
    "    def get_root(self, X):\n",
    "        z = self.encode_z(X)\n",
    "        logits, probs, value_probs = self.ac(z)\n",
    "        \n",
    "        return z.squeeze(1), logits.squeeze(1), probs.squeeze(1), value_probs.squeeze(1)\n",
    "        #return z, logits, probs, value_probs\n",
    "        \n",
    "    def encode(self, X):\n",
    "        batch, seq = X.shape[:2]\n",
    "        self.batch = batch\n",
    "        self.seq = seq\n",
    "        X = self.encoder_cnn(X.contiguous().view(self.batch*self.seq, *(X.shape[2:])))\n",
    "        X = X.contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "        z = X.clone()\n",
    "        \n",
    "        X = X.flatten(-3,-1)\n",
    "        \n",
    "        X = self.projection(X)\n",
    "        return X, z\n",
    "    \n",
    "    def encode_z(self, X):\n",
    "        batch, seq = X.shape[:2]\n",
    "        self.batch = batch\n",
    "        self.seq = seq\n",
    "        X = self.encoder_cnn(X.contiguous().view(self.batch*self.seq, *(X.shape[2:])))\n",
    "        X = X.contiguous().view(self.batch, self.seq, *X.shape[-3:])\n",
    "\n",
    "        return X\n",
    "\n",
    "    def env_step(self, X):\n",
    "        with torch.no_grad():\n",
    "            z = self.encode_z(X)\n",
    "            _, probs, _ = self.ac(z)\n",
    "    \n",
    "    \n",
    "            #return probs.argmax(-1)\n",
    "            return torch.multinomial(probs.squeeze(), 1) \n",
    "            \n",
    "    \n",
    "    def get_zero_ht(self, batch_size):\n",
    "        ht = torch.zeros(batch_size, self.hiddens, device='cuda')\n",
    "        ct = torch.zeros_like(ht)\n",
    "        return (ht, ct)\n",
    "    \n",
    "    def transition_one_step(self, z, action, ht):\n",
    "        \n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action_one_hot = (\n",
    "            torch.ones(\n",
    "                (\n",
    "                    z.shape[0],\n",
    "                    z.shape[2],\n",
    "                    z.shape[3],\n",
    "                )\n",
    "            )\n",
    "            .to(action.device)\n",
    "            .float()\n",
    "        )\n",
    "        \n",
    "        action = (action[:, None, None] * action_one_hot / self.n_actions)[:,None]\n",
    "        #print('one step', z.shape, action.shape)\n",
    "        z_pred = torch.cat( (z, action), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        reward_pred, ht = self.reward_pred.transition_one_step(z_pred, ht)\n",
    "\n",
    "        logits, probs, value_probs = self.ac.one_step(z_pred)\n",
    "        \n",
    "\n",
    "        \n",
    "        return z_pred, logits, probs, value_probs, reward_pred, ht\n",
    "    \n",
    "    def get_transition(self, z, action):\n",
    "        z = z.contiguous().view(-1, *z.shape[-3:])\n",
    "        \n",
    "        action_one_hot = (\n",
    "            torch.ones(\n",
    "                (\n",
    "                    z.shape[0],\n",
    "                    5,\n",
    "                    z.shape[2],\n",
    "                    z.shape[3],\n",
    "                )\n",
    "            )\n",
    "            .to(action.device)\n",
    "            .float()\n",
    "        )\n",
    "        \n",
    "        action = (action[:, :, None, None] * action_one_hot / self.n_actions)[:,:,None]\n",
    "\n",
    "        #print('transition full', z.shape, action.shape)\n",
    "        z_pred = torch.cat( (z, action[:,0]), 1)\n",
    "        z_pred = self.transition(z_pred)\n",
    "        \n",
    "        \n",
    "        z_preds=[z_pred.clone()]\n",
    "        \n",
    "\n",
    "        for k in range(4):\n",
    "            z_pred = torch.cat( (z_pred, action[:,k+1]), 1)\n",
    "            z_pred = self.transition(z_pred)\n",
    "            \n",
    "            \n",
    "            z_preds.append(z_pred)\n",
    "        \n",
    "        \n",
    "        z_pred = torch.stack(z_preds,1)\n",
    "        \n",
    "        reward_pred = self.reward_pred(z_pred)\n",
    "        \n",
    "        #print('transition full z_pred reward_pred', z_pred.shape, reward_pred.shape)\n",
    "\n",
    "        z_proj_pred = self.projection(z_pred.flatten(-3,-1)).view(self.batch,5,-1)\n",
    "        z_proj_pred = self.prediction(z_proj_pred)\n",
    "        \n",
    "        return z_proj_pred, reward_pred\n",
    "\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        z = self.encode_z(X)\n",
    "        values = self.ac(z)[-1]\n",
    "        values = (values*symexp(self.support)).sum(-1)\n",
    "        \n",
    "        return values\n",
    "        \n",
    "    \n",
    "    def network_ema(self, rand_network, target_network, alpha=0.5):\n",
    "        for param, param_target in zip(rand_network.parameters(), target_network.parameters()):\n",
    "            param_target.data = alpha * param_target.data + (1 - alpha) * param.data.clone()\n",
    "\n",
    "    def hard_reset(self, random_model, alpha=0.5):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            self.network_ema(random_model.encoder_cnn, self.encoder_cnn, alpha)\n",
    "            self.network_ema(random_model.transition, self.transition, alpha)\n",
    "\n",
    "            self.network_ema(random_model.projection, self.projection, 0)\n",
    "            self.network_ema(random_model.prediction, self.prediction, 0)\n",
    "            self.network_ema(random_model.reward_mlp, self.reward_mlp, 0)\n",
    "\n",
    "            self.network_ema(random_model.a, self.a, 0)\n",
    "            self.network_ema(random_model.v, self.v, 0)\n",
    "\n",
    "\n",
    "def copy_states(source, target):\n",
    "    for key, _ in zip(source.state_dict()['state'].keys(), target.state_dict()['state'].keys()):\n",
    "\n",
    "        target.state_dict()['state'][key]['exp_avg_sq'] = copy.deepcopy(source.state_dict()['state'][key]['exp_avg_sq'])\n",
    "        target.state_dict()['state'][key]['exp_avg'] = copy.deepcopy(source.state_dict()['state'][key]['exp_avg'])\n",
    "        target.state_dict()['state'][key]['step'] = copy.deepcopy(source.state_dict()['state'][key]['step'])\n",
    "        \n",
    "def target_model_ema(model, model_target, decay=critic_ema_decay):\n",
    "    with torch.no_grad():\n",
    "        for param, param_target in zip(model.parameters(), model_target.parameters()):\n",
    "            param_target.data = decay * param_target.data + (1.0 - decay) * param.data.clone()\n",
    "\n",
    "\n",
    "model=EfficientZero(n_actions).cuda()\n",
    "model_target=EfficientZero(n_actions).cuda()\n",
    "#model_reanalyze=DQN(n_actions).cuda()\n",
    "\n",
    "model_target.load_state_dict(model.state_dict())\n",
    "#model_reanalyze.load_state_dict(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749fc5be-3515-4f22-9f35-74de292e98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS_Node(nsd_Module):\n",
    "    def __init__(self, z, value, logits, probs, ht, reward, prev_state, n_actions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.transitions = [None]*self.n_actions\n",
    "        \n",
    "        self.n = torch.zeros(n_actions, device='cuda')\n",
    "        #print(f\"logits {logits.shape}\")\n",
    "\n",
    "        self.Q = torch.zeros_like(logits)\n",
    "\n",
    "        self.choosen_action = torch.tensor(-1, device='cuda', dtype=torch.long)\n",
    "        \n",
    "    def reset_n(self):\n",
    "        self.n = torch.zeros(self.n_actions, device='cuda')\n",
    "\n",
    "    def get_stats(self):\n",
    "        \n",
    "        return self.Q, self.logits, self.probs, self.value, self.ht, self.z, self.n, self.reward, self.choosen_action\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MCTS(nsd_Module):\n",
    "    def __init__(self, n_actions, k=5, batch_size=3, n_sim=16, topk_actions=8, c_visit=50, c_scale=0.1):\n",
    "        # Good c_scale values are 0.1 and 1\n",
    "        super().__init__()\n",
    "        self.topk_actions = torch.tensor(topk_actions)\n",
    "    \n",
    "\n",
    "    def get_root(self, model, x):\n",
    "        z, logits, probs, value_probs = model.get_root(x)\n",
    "        value = (value_probs*symexp(model.support)).sum(-1)\n",
    "        ###print(f\"get root z value {z.shape, value.shape}\")\n",
    "        ht = torch.zeros(z.shape[0], 512, device='cuda')\n",
    "        '''\n",
    "        print(f\"ROOT ROOT\")\n",
    "        if Q.shape[0]>1:\n",
    "            print(f\"{z.sum(), Q[0]==Q[1]}\")\n",
    "            print(f\"{z.shape, Q.shape}\")\n",
    "        '''\n",
    "        nodes = []\n",
    "        for i in range(z.shape[0]):\n",
    "            root = MCTS_Node(z[i], value[i], logits[i], probs[i], (ht[i], ht[i]), torch.tensor([0]*self.batch_size).cuda()[i], prev_state=None, n_actions=self.n_actions)\n",
    "            nodes.append(root)\n",
    "        self.root = nodes\n",
    "        #print(f\"ROOT ROOT\")\n",
    "        return self.root\n",
    "\n",
    "    def collate_nodes(self):\n",
    "        Q, logits, probs, values, hts, cts, Z, N, R, A = [], [], [], [], [], [], [], [], [], []\n",
    "        for node in self.cur_state:\n",
    "            q, logit, prob, value, ht, z, n, r, a = node.get_stats()\n",
    "            Q.append(q)\n",
    "            logits.append(logit)\n",
    "            probs.append(prob)\n",
    "            values.append(value)\n",
    "            hts.append(ht[0])\n",
    "            cts.append(ht[1])\n",
    "            Z.append(z)\n",
    "            N.append(n)\n",
    "            R.append(r)\n",
    "            A.append(a)\n",
    "            \n",
    "        return torch.stack(Q,0), torch.stack(logits,0), torch.stack(probs,0), torch.stack(values,0), (torch.stack(hts,0), torch.stack(cts,0)), torch.stack(Z,0), \\\n",
    "                torch.stack(N,0), torch.stack(R,0), torch.stack(A,0)\n",
    "    \n",
    "    def transition(self, model, x, action, ht):\n",
    "        '''\n",
    "        print(f\"TRANSITION TRANSITION\")\n",
    "        if x.shape[0]>1:\n",
    "            print(f\"{x[0]==x[1]}\")\n",
    "        '''\n",
    "        \n",
    "        z, logits, probs, value_probs, reward_pred, ht = model.transition_one_step(x, action, ht)\n",
    "        value = (value_probs*symexp(model.support)).sum(-1)\n",
    "        reward_pred = (reward_pred*model.reward_support).sum(-1)\n",
    "        \n",
    "        ###print(f\"mcts transition {ht[0].shape, ht[1].shape}\")\n",
    "        \n",
    "        '''\n",
    "        print(f\"TRANSITION {z.shape, Q.shape, reward_pred.shape}\")\n",
    "        if Q.shape[0]>1:\n",
    "            print(f\"{Q[0]==Q[1]}\")\n",
    "            print(f\"{z[0].sum(), z[1].sum()}\")\n",
    "        '''\n",
    "        nodes = []\n",
    "        \n",
    "        for i in range(z.shape[0]):\n",
    "            if self.cur_state[i].transitions[action[i]] == None:\n",
    "                node = MCTS_Node(z[i], value[i], logits[i], probs[i], (ht[0][i], ht[1][i]), reward_pred[i], prev_state=self.cur_state[i], n_actions=self.n_actions)\n",
    "                nodes.append(node)\n",
    "                self.cur_state[i].transitions[action[i]] = node\n",
    "            else:\n",
    "                nodes.append(self.cur_state[i].transitions[action[i]])\n",
    "                \n",
    "\n",
    "        return nodes\n",
    "\n",
    "\n",
    "    \n",
    "    '''  BACKUP  '''\n",
    "    def backup(self, model):\n",
    "        \n",
    "        q, logits, probs, values, ht, z, n, r_t, choosen_action = self.collate_nodes()\n",
    "        \n",
    "        _, _, value_probs = model.ac.one_step(z)\n",
    "        next_values = (value_probs*symexp(model.support)).sum(-1)\n",
    "        \n",
    "        \n",
    "\n",
    "        gammas = torch.ones(self.batch_size, self.k, device='cuda')*0.997\n",
    "        \n",
    "\n",
    "        for i in range(len(self.cur_state)):\n",
    "            self.cur_state[i] = self.cur_state[i].prev_state\n",
    "            \n",
    "\n",
    "        for l in range(self.k):\n",
    "            Q, logits, probs, values, ht, z, n, r_t, choosen_action = self.collate_nodes()\n",
    "            \n",
    "\n",
    "            ###print(f\"td {(r*gammas[:,:l+1].cumprod(-1)).sum(-1).shape, next_values.shape, (gammas[:,:l+1].prod(-1)).shape}\")\n",
    "            #print(f\"{r_t.shape, (next_values*(gammas[:,:l+1].prod(-1))).shape}\")\n",
    "            \n",
    "            returns = r_t + next_values*(gammas[:,:l+1].prod(-1))\n",
    "            \n",
    "            #print(f\"backup returns {returns.shape}\")\n",
    "            #print(f\"n choosen action {n}\\n{choosen_action}\\n\")\n",
    "            \n",
    "            n_action = n[torch.arange(self.batch_size), choosen_action]\n",
    "            \n",
    "            ###print(f\"Q {Q.shape, n.shape, choosen_action.shape}\")\n",
    "            ###print(f\"{self.batch_size, choosen_action}\")\n",
    "\n",
    "            \n",
    "            #Q[torch.arange(self.batch_size), choosen_action] = (n_action*Q[torch.arange(self.batch_size),choosen_action] + returns) / (n_action+1)\n",
    "            Q[torch.arange(self.batch_size), choosen_action] += returns\n",
    "            \n",
    "            n[torch.arange(self.batch_size), choosen_action] += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            for i in range(len(self.cur_state)):\n",
    "                self.cur_state[i].Q = Q[i]\n",
    "                self.cur_state[i].n = n[i]\n",
    "                \n",
    "                self.cur_state[i] = self.cur_state[i].prev_state\n",
    "                \n",
    "        \n",
    "    def forward(self, model, x, w_gumbel=1):\n",
    "        with torch.no_grad():\n",
    "    \n",
    "            self.cur_state = self.get_root(model, x)\n",
    "    \n",
    "            logits = self.collate_nodes()[1]\n",
    "            gumbel = F.gumbel_softmax(torch.zeros_like(logits)) * w_gumbel\n",
    "            gumbel_sa = gumbel+logits\n",
    "            \n",
    "    \n",
    "            k = min(self.topk_actions, self.n_actions)\n",
    "            \n",
    "            Q_mask = F.one_hot(gumbel_sa.topk(k)[1], self.n_actions).sum(-2)\n",
    "            #print(f\"TOP K {gumbel_sa.topk(k)[1]}\")\n",
    "            #print(f\"STARTING Q MASK {Q_mask}\")\n",
    "            \n",
    "            \n",
    "    \n",
    "            halve_sims = self.n_sim//2\n",
    "            halves = 2\n",
    "    \n",
    "            next_halve = math.floor(self.n_sim/(torch.log2(self.topk_actions)*(self.topk_actions)))\n",
    "\n",
    "            \n",
    "            for sim in range(self.n_sim):\n",
    "                actions_to_step = []\n",
    "                improved_policies = []\n",
    "                for l in range(self.k):\n",
    "                    q, logits, probs, values, ht, z, n, _, _ = self.collate_nodes()\n",
    "                    \n",
    "                    if l==0 and sim==next_halve:\n",
    "                        gumbel = F.gumbel_softmax(torch.zeros_like(logits)) * w_gumbel\n",
    "                        \n",
    "                        gumbel_sa = gumbel + logits\n",
    "                        k = min(self.topk_actions//halves*2, self.n_actions)\n",
    "                        \n",
    "                        \n",
    "                        Q_mask = F.one_hot(gumbel_sa.topk(k)[1], self.n_actions).sum(-2)\n",
    "                        \n",
    "    \n",
    "                        next_halve += math.floor(self.n_sim/(torch.log2(self.topk_actions)*(self.topk_actions/(halves*2))))\n",
    "                        \n",
    "                        halves*=2\n",
    "\n",
    "                    v_mix = (1/(1+n.sum(-1))) * ( values + (n.sum(-1)/(((n!=0)*probs+0.01).sum(-1)) ) * ((n!=0)*q*probs).sum(-1) )\n",
    "                    v_mix = v_mix[:,None].repeat_interleave(q.shape[-1], -1)\n",
    "                        \n",
    "                    completed_Q = (n==0)*v_mix + (n>0)*q\n",
    "                    \n",
    "                    \n",
    "                    completed_Q = (self.c_visit + n.max(-1)[0][:,None])*self.c_scale*completed_Q\n",
    "\n",
    "                    improved_policy = F.softmax((logits+completed_Q),-1)\n",
    "                    improved_policies.append(improved_policy)\n",
    "\n",
    "                    '''\n",
    "                    if q.shape[0]>1:\n",
    "                        print(f\"Mask {Q_mask}\")\n",
    "                        print(f\"Q {q}\")\n",
    "                        print(f\"completed Q {completed_Q}\")\n",
    "                        print(f\"imp. policy {improved_policy}\")\n",
    "                        print(f\"\")\n",
    "                    '''\n",
    "                        \n",
    "                    if l==0:\n",
    "                        Q = gumbel_sa + (self.c_visit + n.max(-1)[0][:,None])*self.c_scale*q/(sim+1)\n",
    "\n",
    "                        action = (Q*Q_mask).argmax(-1) \n",
    "                        \n",
    "                    else:\n",
    "                        Q = improved_policy - n/(1+n.sum(-1)[:,None])\n",
    "    \n",
    "                        action = Q.argmax(-1)\n",
    "                        \n",
    "    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    actions_to_step.append(Q.argmax(-1))\n",
    "                    \n",
    "                    \n",
    "                    for i in range(len(self.cur_state)):\n",
    "                        self.cur_state[i].choosen_action = action[i]\n",
    "                        \n",
    "                    self.cur_state = self.transition(model, z, action, ht)\n",
    "                    \n",
    "                self.backup(model)\n",
    "                self.cur_state = self.root\n",
    "                \n",
    "            \n",
    "            Q, logits, probs, values, ht, z, n, _, _ = self.collate_nodes()\n",
    "\n",
    "\n",
    "            \n",
    "            '''\n",
    "            v_mix = (1/(1+n.sum(-1))) * ( values + (n.sum(-1)/(((n!=0)*probs+0.01).sum(-1)) ) * ((n!=0)*Q*probs).sum(-1) )\n",
    "            v_mix = v_mix[:,None].repeat_interleave(Q.shape[-1], -1)\n",
    "            \n",
    "            completed_Q = (n==0)*v_mix + (n>0)*Q\n",
    "            \n",
    "            \n",
    "            completed_Q = (self.c_visit + n.max(-1)[0][:,None])*self.c_scale*completed_Q\n",
    "                        \n",
    "                        \n",
    "            improved_policy = F.softmax((logits+completed_Q),-1)\n",
    "            '''\n",
    "\n",
    "            \n",
    "            return Q.max(-1)[0]/self.n_sim, torch.stack(improved_policies,1), [torch.multinomial(improved_policy,1)]\n",
    "\n",
    "\n",
    "mcts           = MCTS(n_actions, n_sim = n_sim, topk_actions = topk_actions, batch_size = batch_size)\n",
    "mcts_inference = MCTS(n_actions, n_sim = n_sim, topk_actions = topk_actions, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ca1d0e5-d128-428b-bcbc-be68dccc005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "perception_modules=[model.encoder_cnn, model.transition]\n",
    "actor_modules=[model.prediction, model.projection, model.ac, model.reward_pred]\n",
    "\n",
    "params_wm=[]\n",
    "for module in perception_modules:\n",
    "    for param in module.parameters():\n",
    "        if param.requires_grad==True: # They all require grad\n",
    "            params_wm.append(param)\n",
    "\n",
    "params_ac=[]\n",
    "for module in actor_modules:\n",
    "    for param in module.parameters():\n",
    "        if param.requires_grad==True:\n",
    "            params_ac.append(param)\n",
    "\n",
    "\n",
    "#optimizer = torch.optim.AdamW(chain(params_wm, params_ac),\n",
    "#                                lr=lr, weight_decay=1e-4)\n",
    "optimizer = torch.optim.SGD(chain(params_wm, params_ac), momentum=0.9,\n",
    "                                lr=0.2, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f2a7d7-81c6-4c93-8cbe-1f15fa75af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "                         transforms.Resize((96,96)),\n",
    "                        ])\n",
    "\n",
    "\n",
    "def preprocess(state):\n",
    "    state=torch.tensor(state, dtype=torch.float, device='cuda') / 255\n",
    "    state=train_tfms(state.permute(0,3,1,2))\n",
    "    return state\n",
    "\n",
    "# https://github.com/google/dopamine/blob/master/dopamine/jax/agents/dqn/dqn_agent.py\n",
    "def linearly_decaying_epsilon(decay_period, step, warmup_steps, epsilon):\n",
    "    steps_left = decay_period + warmup_steps - step\n",
    "    bonus = (1.0 - epsilon) * steps_left / decay_period\n",
    "    bonus = np.clip(bonus, 0., 1. - epsilon)\n",
    "    return epsilon + bonus\n",
    "\n",
    "\n",
    "#def epsilon_greedy(Q_action, step, final_eps=0, num_envs=1):\n",
    "def epsilon_greedy(actions_to_step, step, final_eps=0, num_envs=1):\n",
    "    epsilon = linearly_decaying_epsilon(2001, step, 2000, final_eps)\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        #action = torch.randint(0, n_actions, (num_envs,), dtype=torch.int64, device='cuda').squeeze(0)\n",
    "        action = torch.randint(0, n_actions, (num_envs,), dtype=torch.int64).squeeze(0)\n",
    "    else:\n",
    "        #action = Q_action.view(num_envs).squeeze(0).to(torch.int64)\n",
    "        action = actions_to_step.pop(0).squeeze()\n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6722829c-d4bc-4d9f-aa35-80f6133a4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = torch.nn.MSELoss(reduction='none')\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "def optimize(step, grad_step, n):\n",
    "        \n",
    "    model.train()\n",
    "    model_target.train()\n",
    "\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16, enabled=False):\n",
    "        with torch.no_grad():\n",
    "            states, next_states, rewards, action, c_flag, idxs, is_w = memory.sample(n, batch_size, grad_step)\n",
    "            z = model_target.encode(states[:,1:6])[0]\n",
    "            \n",
    "        terminal=1-c_flag\n",
    "        #print(f\"STUFF HERE {states.shape, rewards.shape, c_flag.shape, action.shape, n}\")\n",
    "    \n",
    "        z_proj, z_proj_pred, reward_pred, logits, probs, value_probs = model(states[:,:5], action[:,:5].long())\n",
    "        \n",
    "        value = (value_probs*symexp(model.support)).sum(-1).detach().squeeze()\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        next_values = model_target.evaluate(next_states[:,n-1][:,None].contiguous())\n",
    "        \n",
    "\n",
    "        #action = action[:,0,None].expand(batch_size,num_buckets)\n",
    "        #action = action[:,None]\n",
    "        action = F.one_hot(action[:,0], n_actions)\n",
    "        with torch.no_grad():\n",
    "            gammas_one=torch.ones(batch_size,n,1,dtype=torch.float, device='cuda')\n",
    "            gamma_step = 1-torch.tensor(( (schedule_max_step - min(grad_step, schedule_max_step)) / schedule_max_step) * (initial_gamma-final_gamma) + final_gamma).exp()\n",
    "            gammas=gammas_one*gamma_step\n",
    "\n",
    "            \n",
    "            returns = []\n",
    "            for t in range(n):\n",
    "                ret = 0\n",
    "                for u in reversed(range(t, n)):\n",
    "                    ret += torch.prod(c_flag[:,t+1:u+1],-2)*torch.prod(gammas[:,t:u],-2)*rewards[:,u+1]\n",
    "                returns.append(ret)\n",
    "            returns = torch.stack(returns,1)\n",
    "        \n",
    "        plot_vs = returns.clone().sum(-1)\n",
    "        \n",
    "        same_traj = (torch.prod(c_flag[:,:n],-2)).squeeze()\n",
    "        loss = 0\n",
    "\n",
    "\n",
    "\n",
    "        '''Value non-SVE'''\n",
    "        value_prefix = returns[:,:n]\n",
    "        returns = returns[:,0]\n",
    "        #returns = returns + torch.prod(gammas[0,:initial_n],-2).squeeze()*same_traj[:,None]*model.support[None,:]\n",
    "        #returns = returns.squeeze()\n",
    "            \n",
    "        #next_values = next_values[:,0]\n",
    "\n",
    "\n",
    "        returns = returns + torch.prod(c_flag[:,:n],-2) * torch.prod(gammas[:,:n],-2) * next_values\n",
    "\n",
    "        th_returns = two_hot_view(returns, 51, model.support)\n",
    "        \n",
    "        loss += -(th_returns*torch.log(value_probs[:,0].squeeze()+eps)).sum(-1) * 0.25\n",
    "\n",
    "        #loss += -(action*torch.log(probs.squeeze()+eps)).sum(-1) * (returns.squeeze()-value)\n",
    "        \n",
    "\n",
    "        '''MCTS'''\n",
    "        value_mcts, improved_policy, _ = mcts(model_target, states[:,0][:,None])\n",
    "        \n",
    "        if step > reset_every*0.4:\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "            steps = idxs - reset_every*(idxs//reset_every)\n",
    "            sve_mask = (steps<reset_every*0.4).float() + (steps>reset_every*0.8).float()\n",
    "            sve_mask = sve_mask.cuda()\n",
    "            \n",
    "            #print(f\"{sve_mask, idxs}\")\n",
    "\n",
    "            loss = sve_mask*loss\n",
    "\n",
    "            #print(f\"{value_mcts.shape, value_probs.shape}\")\n",
    "            value_mcts_th = two_hot(value_mcts, 51, model.support)\n",
    "            loss += -(value_mcts_th*torch.log(value_probs[:,0].squeeze()+eps)).sum(-1) * (1-sve_mask) * 0.25\n",
    "            \n",
    "            #print(f\"optim improved policy, probs {improved_policy.shape, probs.shape}\")\n",
    "        \n",
    "        loss += -(improved_policy*torch.log(probs.squeeze()+eps)).sum(-1).mean(-1)# * (1-sve_mask)\n",
    "        ### print(f\"{improved_policy[0], probs[0].squeeze()}\")\n",
    "        \n",
    "        \n",
    "        dqn_loss = loss.clone().mean()\n",
    "        \n",
    "        '''Reward Pred'''\n",
    "\n",
    "        value_prefix_th = two_hot_view_no_symlog(value_prefix.squeeze().clip(-2,2), 51, model.reward_support)\n",
    "        value_prefix_th = value_prefix_th.view(batch_size,-1,51)\n",
    "        \n",
    "        \n",
    "        loss += -(value_prefix_th*torch.log(reward_pred+eps)).sum(-1).mean(-1)\n",
    "        \n",
    "        batched_loss = loss.clone()\n",
    "\n",
    "\n",
    "        '''Entropy Term'''\n",
    "        loss += 5e-3 * (probs.squeeze() * torch.log(probs.squeeze()+eps)).sum(-1).mean(-1)\n",
    "\n",
    "        \n",
    "        '''Recon'''\n",
    "        z = F.normalize(z, 2, dim=-1, eps=1e-5)\n",
    "        z_proj_pred = F.normalize(z_proj_pred, 2, dim=-1, eps=1e-5)\n",
    "\n",
    "        \n",
    "        recon_loss = (mse(z_proj_pred.contiguous().view(-1,1024), z.contiguous().view(-1,1024))).sum(-1)\n",
    "        recon_loss = 2*(recon_loss.view(batch_size, -1).mean(-1))*same_traj\n",
    "        \n",
    "        \n",
    "        loss += recon_loss\n",
    "\n",
    "        \n",
    "        loss = (loss*is_w).mean() # mean across batch axis\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    param_norm, grad_norm = params_and_grad_norm(model)\n",
    "    #scaler.scale(loss).backward()\n",
    "    #scaler.unscale_(optimizer)\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
    "    #scaler.step(optimizer)\n",
    "    #scaler.update()\n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #memory.set_priority(idxs, batched_loss)\n",
    "    memory.set_priority(idxs, batched_loss, same_traj)\n",
    "    \n",
    "    \n",
    "    lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    wandb.log({'loss': loss, 'dqn_loss': dqn_loss, 'recon_loss': recon_loss.mean(), 'lr': lr, 'returns': plot_vs.mean(),\n",
    "               'buffer rewards': rewards.mean(0).sum(), 'is_w': is_w.mean(),\n",
    "               'gamma': gamma_step, 'param_norm': param_norm.sum(), 'grad_norm': grad_norm.sum()})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "scores=[]\n",
    "memory.free()\n",
    "step=0\n",
    "#model.share_memory()\n",
    "grad_step=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05991348-a11a-43bd-882f-02df107fc543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2002/102000 [00:19<16:38, 100.14it/s]C:\\Users\\sneep\\AppData\\Local\\Temp\\ipykernel_3312\\1522337271.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gamma_step = 1-torch.tensor(( (schedule_max_step - min(grad_step, schedule_max_step)) / schedule_max_step) * (initial_gamma-final_gamma) + final_gamma).exp()\n",
      "  4%|▎         | 3790/102000 [2:41:07<137:08:01,  5.03s/it]\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step=0\n",
    "\n",
    "progress_bar = tqdm.tqdm(total=total_steps)\n",
    "\n",
    "while step<(80000):\n",
    "    state, info = env.reset()\n",
    "    state = preprocess(state)\n",
    "\n",
    "    states = deque(maxlen=4)\n",
    "    for i in range(4):\n",
    "        states.append(state)\n",
    "    \n",
    "    \n",
    "    eps_reward=torch.tensor([0], dtype=torch.float)\n",
    "    \n",
    "    reward=np.array([0])\n",
    "    done_flag=np.array([False])\n",
    "    terminated=np.array([False])\n",
    "\n",
    "    last_lives=np.array([0])\n",
    "    life_loss=np.array([0])\n",
    "    resetted=np.array([0])\n",
    "\n",
    "    actions_to_step = []\n",
    "    \n",
    "    last_grad_update=0\n",
    "    while step<(total_steps):\n",
    "        progress_bar.update(1)\n",
    "        model_target.train()\n",
    "        \n",
    "        len_memory = len(memory)\n",
    "        #if resetted[0]>0:\n",
    "        #    states = env.noop_steps(states)\n",
    "\n",
    "        \n",
    "        if len(actions_to_step)==0:\n",
    "            actions_to_step = mcts_inference(model, torch.cat(list(states),-3).unsqueeze(0), w_gumbel=0)[-1]\n",
    "        '''\n",
    "        if len(actions_to_step)==0:\n",
    "            if step > reset_every*0.4: \n",
    "                _, _, actions_to_step = mcts_inference(model, torch.cat(list(states),-3).unsqueeze(0))\n",
    "            else:\n",
    "                #Q_action = model_target.env_step(torch.cat(list(states),-3).unsqueeze(0))\n",
    "                Q_action = model.env_step(torch.cat(list(states),-3).unsqueeze(0))\n",
    "                actions_to_step = [Q_action]\n",
    "                #print(f\"ENV STEP CALLED {actions_to_step}\")\n",
    "        '''\n",
    "        action = epsilon_greedy(actions_to_step, len_memory).cpu()\n",
    "        \n",
    "        \n",
    "        memory.push(torch.cat(list(states),-3).detach().cpu(), torch.tensor(reward,dtype=torch.float), action,\n",
    "                    torch.tensor(np.logical_or(done_flag, life_loss),dtype=torch.bool))\n",
    "        #print('action', action, action.shape)\n",
    "        \n",
    "        state, reward, terminated, truncated, info = env.step([action.numpy()])\n",
    "        state = preprocess(state)\n",
    "        states.append(state)\n",
    "        \n",
    "        eps_reward+=reward\n",
    "        reward = reward.clip(-1, 1)\n",
    "\n",
    "\n",
    "        if grad_step%400==0:\n",
    "            model_target.load_state_dict(model.state_dict())\n",
    "        \n",
    "        done_flag = np.logical_or(terminated, truncated)\n",
    "        lives = info['lives']\n",
    "        life_loss = (last_lives-lives).clip(min=0)\n",
    "        resetted = (lives-last_lives).clip(min=0)\n",
    "        last_lives = lives\n",
    "\n",
    "        \n",
    "        n = int(initial_n * (final_n/initial_n)**(min(grad_step,schedule_max_step) / schedule_max_step))\n",
    "        n = np.array(n).item()\n",
    "        \n",
    "        \n",
    "\n",
    "        if len_memory>2000:\n",
    "            for i in range(1):\n",
    "                optimize(step, grad_step, n)\n",
    "                \n",
    "                #target_model_ema(model, model_target)\n",
    "                \n",
    "                #target_model_ema(model, model_reanalyze, decay=0.98)\n",
    "                \n",
    "                grad_step+=1\n",
    "\n",
    "        \n",
    "        if ((step+1)%10000)==0:\n",
    "            save_checkpoint(model, model_target, optimizer, step,\n",
    "                            'checkpoints/atari_last.pth')\n",
    "        \n",
    "            \n",
    "        \n",
    "        #if grad_step>reset_every:\n",
    "        if grad_step>200000:\n",
    "            #eval()\n",
    "            print('Reseting on step', step, grad_step)\n",
    "            \n",
    "            random_model = DQN(n_actions).cuda()\n",
    "            model.hard_reset(random_model)\n",
    "            \n",
    "            random_model = DQN(n_actions).cuda()\n",
    "            model_target.hard_reset(random_model)\n",
    "\n",
    "            #random_model = DQN(n_actions).cuda()\n",
    "            #model_reanalyze.hard_reset(random_model)\n",
    "            random_model=None\n",
    "            \n",
    "            seed_np_torch(SEED)\n",
    "            \n",
    "            grad_step=0\n",
    "\n",
    "            actor_modules=[model.prediction, model.projection, model.ac, model.reward_pred]\n",
    "            params_ac=[]\n",
    "            for module in actor_modules:\n",
    "                for param in module.parameters():\n",
    "                    params_ac.append(param)\n",
    "                    \n",
    "\n",
    "            perception_modules=[model.encoder_cnn, model.transition]\n",
    "            params_wm=[]\n",
    "            for module in perception_modules:\n",
    "                for param in module.parameters():\n",
    "                    params_wm.append(param)\n",
    "            \n",
    "            #optimizer_aux = torch.optim.AdamW(params_wm, lr=lr, weight_decay=1e-4)\n",
    "            optimizer_aux = torch.optim.SGD(params_wm, lr=0.2, momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            copy_states(optimizer, optimizer_aux)\n",
    "            \n",
    "            #optimizer = torch.optim.AdamW(chain(params_wm, params_ac),\n",
    "            #                    lr=lr, weight_decay=1e-4)\n",
    "            optimizer = torch.optim.SGD(chain(params_wm, params_ac), momentum=0.9,\n",
    "                                lr=0.2, weight_decay=1e-4)\n",
    "            copy_states(optimizer_aux, optimizer)\n",
    "        \n",
    "        \n",
    "        \n",
    "        step+=1\n",
    "        \n",
    "        log_t = done_flag.astype(float).nonzero()[0]\n",
    "        \n",
    "        if len(log_t)>0:\n",
    "            for log in log_t:\n",
    "                wandb.log({'eps_reward': eps_reward[log].sum()})\n",
    "                scores.append(eps_reward[log].clone())\n",
    "            eps_reward[log_t]=0\n",
    "\n",
    "save_checkpoint(model, model_target, optimizer, step, f'checkpoints/effz_{env_name}_{SEED}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87344597-e733-451c-8a5f-2a00b5511061",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_to_eval=False\n",
    "\n",
    "if load_to_eval:\n",
    "    model.load_state_dict(torch.load(f'checkpoints/effz_{env_name}_{SEED}.pth')['model_state_dict'])\n",
    "    model_target.load_state_dict(torch.load(f'checkpoints/effz_{env_name}_{SEED}.pth')['model_target_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d36218-ab48-42d9-a7ba-fabb66cc758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_envs=1\n",
    "\n",
    "#env = gym.vector.make(f\"{env_name}NoFrameskip-v4\", num_envs=num_envs, render_mode='human')\n",
    "env = gym.vector.make(f\"{env_name}NoFrameskip-v4\", num_envs=num_envs)\n",
    "env = MaxLast2FrameSkipWrapper(env, seed=SEED)\n",
    "\n",
    "def eval_phase(eval_runs=50, max_eval_steps=27000, num_envs=1):\n",
    "    progress_bar = tqdm.tqdm(total=eval_runs)\n",
    "    \n",
    "    scores=[]\n",
    "    \n",
    "    state, info = env.reset()\n",
    "    state = preprocess(state)\n",
    "    print(f\"init state {state.shape}\")\n",
    "    \n",
    "    states = deque(maxlen=4)\n",
    "    for i in range(4):\n",
    "        states.append(state)\n",
    "    \n",
    "    \n",
    "    eps_reward=torch.tensor([0]*num_envs, dtype=torch.float)\n",
    "    \n",
    "    reward=np.array([0]*num_envs)\n",
    "    terminated=np.array([False]*num_envs)\n",
    "    \n",
    "    last_lives=np.array([0]*num_envs)\n",
    "    life_loss=np.array([0]*num_envs)\n",
    "    resetted=np.array([0])\n",
    "\n",
    "    finished_envs=np.array([False]*num_envs)\n",
    "    done_flag=0\n",
    "    last_grad_update=0\n",
    "    eval_run=0\n",
    "    step=np.array([0]*num_envs)\n",
    "    \n",
    "    actions_to_step=[]\n",
    "    \n",
    "    while eval_run<eval_runs:\n",
    "        #seed_np_torch(SEED+eval_run)\n",
    "        env.seed=SEED+eval_run\n",
    "        model_target.train()\n",
    "        \n",
    "        #if resetted[0]>0:\n",
    "        #    states = env.noop_steps(states)\n",
    "        \n",
    "        #Q_action = model_target.env_step(torch.cat(list(states),-3).unsqueeze(0))\n",
    "        #action = epsilon_greedy(Q_action.squeeze(), 5000, 0.0005, num_envs).cpu()\n",
    "        if len(actions_to_step)==0:\n",
    "            actions_to_step = mcts_inference(model, torch.cat(list(states),-3).unsqueeze(0), w_gumbel=0)[-1]\n",
    "        action = epsilon_greedy(actions_to_step, 5000, 0.0005, num_envs).cpu()\n",
    "        \n",
    "        state, reward, terminated, truncated, info = env.step([action.numpy()] if num_envs==1 else action.numpy())\n",
    "        state = preprocess(state)\n",
    "        states.append(state)\n",
    "        \n",
    "        eps_reward+=reward\n",
    "\n",
    "        \n",
    "        done_flag = np.logical_or(terminated, truncated)\n",
    "        lives = info['lives']\n",
    "        life_loss = (last_lives-lives).clip(min=0)\n",
    "        resetted = (lives-last_lives).clip(min=0)\n",
    "        last_lives = lives        \n",
    "        \n",
    "        step+=1\n",
    "        \n",
    "        log_t = done_flag.astype(float).nonzero()[0]\n",
    "        if len(log_t)>0:# or (step>max_eval_steps).any():\n",
    "            progress_bar.update(1)\n",
    "            for log in log_t:\n",
    "                if finished_envs[log]==False:\n",
    "                    scores.append(eps_reward[log].clone())\n",
    "                    eval_run+=1\n",
    "                    #finished_envs[log]=True\n",
    "                step[log]=0\n",
    "                \n",
    "            eps_reward[log_t]=0            \n",
    "            for i, log in enumerate(step>max_eval_steps):\n",
    "                if log==True and finished_envs[i]==False:\n",
    "                    scores.append(eps_reward[i].clone())\n",
    "                    step[i]=0\n",
    "                    eval_run+=1\n",
    "                    eps_reward[i]=0\n",
    "                    #finished_envs[i]=True\n",
    "            \n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "def eval(eval_runs=50, max_eval_steps=27000, num_envs=1):\n",
    "    assert num_envs==1, 'The code for num eval envs > 1 is messed up.'\n",
    "    \n",
    "    scores = eval_phase(eval_runs, max_eval_steps, num_envs)    \n",
    "    scores = torch.stack(scores)\n",
    "    scores, _ = scores.sort()\n",
    "    \n",
    "    _25th = eval_runs//4\n",
    "\n",
    "    iq = scores[_25th:-_25th]\n",
    "    iqm = iq.mean()\n",
    "    iqs = iq.std()\n",
    "\n",
    "    print(f\"Scores Mean {scores.mean()}\")\n",
    "    print(f\"Inter Quantile Mean {iqm}\")\n",
    "    print(f\"Inter Quantile STD {iqs}\")\n",
    "\n",
    "    \n",
    "    plt.xlabel('Episode (Sorted by Reward)')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(scores)\n",
    "    \n",
    "    new_row = {'env_name': env_name, 'mean': scores.mean().item(), 'iqm': iqm.item(), 'std': iqs.item(), 'seed': SEED}\n",
    "    add_to_csv('results.csv', new_row)\n",
    "\n",
    "    with open(f'results/{env_name}-{SEED}.txt', 'w') as f:\n",
    "        f.write(f\" Scores Mean {scores.mean()}\\n Inter Quantile Mean {iqm}\\n Inter Quantile STD {iqs}\")\n",
    "    \n",
    "    \n",
    "    return scores\n",
    "\n",
    "scores = eval(eval_runs=100, num_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57556912-f811-4eb5-9030-f8f77e3962ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57c70a-3cdb-4c49-a7f7-aa3c77a7e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "new_row = {'env_name': \"Amidar\", 'mean': 11.0, 'iqm': 11.0, 'std': 11.0, 'seed': 000}\n",
    "\n",
    "df = pd.read_csv('results.csv',sep=',')\n",
    "df.loc[len(df.index)] = new_row    \n",
    "#df.to_csv('results.csv', index=False)\n",
    "\n",
    "df\n",
    "'''\n",
    "# Add to csv suddenly stopped working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215ce8f-b10d-4ed3-8651-86ae074d14af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d152c7-0cf1-4c2e-bbc2-dd4abbdd3dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
